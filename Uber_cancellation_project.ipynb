{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5a4ca-2645-4cd2-9c55-de7f693f6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5948d-a058-4d4f-857e-cb0c192d650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Synthetic_Ride_Data_6000.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b5778-531b-4b3e-b48c-b6dcc1368823",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0b06d-29b6-4667-8bab-b3317ac5f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf2dc8-0e8f-4b1a-9d0e-377d2a714f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d80630-36d1-47ea-8817-2bf0a91c7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f2e0c-ac15-47a7-b861-7eec88e26a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8bbd4a-f3a2-4399-b6f4-1f2132f7bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d3259-962b-4178-bed5-abf3daec4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# #dask lib helps in parllel processing and run large datset\n",
    "# # Load the dataset using Dask\n",
    "# file_path = 'Synthetic_Ride_Data_6000.csv'\n",
    "# df = dd.read_csv(file_path)\n",
    "\n",
    "# # Display basic information\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047241f1-a37c-4218-ad1e-3c21c7ea2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancellation_rate = data[data['Cancellation_Status'] == 'Cancelled'].shape[0] / data.shape[0]\n",
    "print(f\"Cancellation Rate: {cancellation_rate:.2%}\")\n",
    "#cancellation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac270f84-96df-45f6-9ebe-311c5aed5e8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Ride_Start_Time']=pd.to_datetime(data['Ride_Start_Time'])\n",
    "data['Ride_End_Time']=pd.to_datetime(data['Ride_End_Time'])\n",
    "plt.figure(figsize=[10,11])\n",
    "sns.countplot(data=data[data['Cancellation_Status']=='Cancelled'],x='Cancellation_Reason', palette='coolwarm')\n",
    "plt.title('cancellation by reason')\n",
    "plt.ylabel('cancellation count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84afdbb-5397-4f99-ae0b-bbced60212ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Ride_Start_Time and Ride_End_Time to datetime format\n",
    "data['Ride_Start_Time'] = pd.to_datetime(data['Ride_Start_Time'], errors='coerce')  # Ensure proper conversion\n",
    "data['Ride_End_Time'] = pd.to_datetime(data['Ride_End_Time'], errors='coerce')  # Ensure proper conversion\n",
    "\n",
    "# Verify if the conversion was successful\n",
    "print(data['Ride_Start_Time'].dtype)\n",
    "\n",
    "# Ensure Hour_of_Ride column is created correctly\n",
    "data['Hour_of_Ride'] = data['Ride_Start_Time'].dt.hour\n",
    "\n",
    "# Cancellation Status Analysis\n",
    "cancel_status_counts = data['Cancellation_Status'].value_counts()\n",
    "print(cancel_status_counts)\n",
    "\n",
    "# Plot cancellation status distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Cancellation_Status', data=data, palette='viridis')\n",
    "plt.title('Cancellation Status Distribution')\n",
    "plt.xlabel('Cancellation Status')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Cancellation reasons analysis\n",
    "cancel_reasons_counts = data['Cancellation_Reason'].value_counts()\n",
    "print(cancel_reasons_counts)\n",
    "\n",
    "# Plot cancellation reasons\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(y='Cancellation_Reason', data=data, palette='viridis', order=data['Cancellation_Reason'].value_counts().index)\n",
    "plt.title('Cancellation Reasons Distribution')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Cancellation Reason')\n",
    "plt.show()\n",
    "\n",
    "# Ride Fare Distribution (for cancelled vs completed rides)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Cancellation_Status', y='Ride_Fare', data=data, palette='viridis')\n",
    "plt.title('Ride Fare Distribution: Cancelled vs Completed')\n",
    "plt.xlabel('Cancellation Status')\n",
    "plt.ylabel('Ride Fare')\n",
    "plt.show()\n",
    "\n",
    "# Analyze the distribution of ride start times (hourly)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Hour_of_Ride', data=data, palette='viridis')\n",
    "plt.title('Distribution of Ride Start Times by Hour')\n",
    "plt.xlabel('Hour of Ride')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Cancellation status by time of day (morning, afternoon, evening)\n",
    "data['Time_of_Day'] = pd.cut(data['Hour_of_Ride'], bins=[0, 6, 12, 18, 24], labels=['Morning', 'Afternoon', 'Evening', 'Night'])\n",
    "time_of_day_cancellation = data.groupby(['Time_of_Day', 'Cancellation_Status']).size().unstack()\n",
    "\n",
    "# Plot cancellation status by time of day\n",
    "time_of_day_cancellation.plot(kind='bar', stacked=True, figsize=(8, 6), colormap='viridis')\n",
    "plt.title('Cancellation Status by Time of Day')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93cd8c-1dce-4dff-9afc-f4c8301adc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ride_Duration'] = (data['Ride_End_Time'] - data['Ride_Start_Time']).dt.total_seconds() / 60.0\n",
    "data.head()\n",
    "#Extract time-based features\n",
    "data['Hour_of_Day'] = data['Ride_Start_Time'].dt.hour\n",
    "data['Day_of_Week'] = data['Ride_Start_Time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b3619-cd87-480a-a427-0f59b710293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "label_encoder = LabelEncoder()\n",
    "data['Cancellation_Reason'] = label_encoder.fit_transform(data['Cancellation_Reason'])\n",
    "#data['User_Type'] = label_encoder.fit_transform(data['User_Type'])\n",
    "data['Driver_ID'] = label_encoder.fit_transform(data['Driver_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faac20f-d560-4b3f-b571-1fbd59711d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Features and Target\n",
    "target = 'Cancellation_Status'\n",
    "features = ['Ride_Duration', 'Hour_of_Day', 'Day_of_Week', 'Cancellation_Reason', 'Driver_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f700601-509e-4f50-a38d-1076921dd0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[features]\n",
    "y = label_encoder.fit_transform(data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15a029-afbc-485f-93ec-2ca15296f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb33a7-df69-404d-baf2-f13e2fdf3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17dc550-0958-4f70-9901-4e4e86939887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Train the Model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c2200-8e28-4b03-8ff7-15aea183bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Feature Importance\n",
    "feature_importances = model.feature_importances_\n",
    "feature_names = features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importances, y=feature_names, palette='viridis')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17627d-aacb-407c-8ae1-af5941e2b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e025a-3791-446f-b8c6-f564fe85e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the model\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,        # Number of trees\n",
    "    max_depth=6,             # Maximum depth of trees\n",
    "    learning_rate=0.1,       # Step size for weight updates\n",
    "    subsample=0.8,           # Fraction of samples used for each tree\n",
    "    colsample_bytree=0.8,    # Fraction of features used for each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e14cb-b9dd-443e-8556-51c9e0d83d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50434b02-6fc9-457f-83c7-dfd3308dcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances, y=feature_names)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b9bc1-a6db-49a9-8ae0-09ea978f1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a new scenario\n",
    "new_scenario = pd.DataFrame({\n",
    "    \"ride_time_minutes\": [30],\n",
    "    \"driver_rating\": [4.5],\n",
    "    \"passenger_rating\": [3.8],\n",
    "    \"ride_fare\": [20],\n",
    "    \"cancellation_reason_encoded\": [le_reason.transform([\"Driver Late\"])[0]]\n",
    "})\n",
    "\n",
    "# Scale the new scenario\n",
    "new_scenario_scaled = scaler.transform(new_scenario)\n",
    "\n",
    "# Predict the cancellation likelihood\n",
    "cancellation_prob = model.predict_proba(new_scenario_scaled)[:, 1][0]\n",
    "print(f\"Probability of Cancellation: {cancellation_prob:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee540129-b77e-42f6-8be3-47c240f2c128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9cd86b-99d6-4a73-b567-1bb4a9744010",
   "metadata": {},
   "source": [
    "Model Generalisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c20f8-2d92-4dc9-9014-071ca9445697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# openai.api_key = \"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\"\n",
    "\n",
    "# def suggest_optimizations(ride_data):\n",
    "#     prompt = f\"\"\"\n",
    "#     Given the following ride details: {ride_data},\n",
    "#     suggest ways to prevent cancellations. Consider better routes, time adjustments, or incentives.\n",
    "#     \"\"\"\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         # model=\"gpt-3.5-turbo\",\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#     )\n",
    "#     return response['choices'][0]['message']['content']\n",
    "\n",
    "# # Example usage\n",
    "# ride_sample = {\n",
    "#     \"pickup_location\": \"Downtown\",\n",
    "#     \"dropoff_location\": \"Airport\",\n",
    "#     \"driver_rating\": 4.2,\n",
    "#     \"passenger_rating\": 4.8,\n",
    "#     \"time_of_request\": \"2025-01-28 15:00:00\"\n",
    "# }\n",
    "# print(suggest_optimizations(ride_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489fb11f-d7f3-4a64-980f-87e465c0771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def generate_scenarios(n=1000):\n",
    "    scenarios = []\n",
    "    for _ in range(n):\n",
    "        scenario = {\n",
    "            \"pickup_location\": fake.city(),\n",
    "            \"dropoff_location\": fake.city(),\n",
    "            \"driver_rating\": round(fake.random.uniform(3.0, 5.0), 1),\n",
    "            \"passenger_rating\": round(fake.random.uniform(3.0, 5.0), 1),\n",
    "            \"time_of_request\": fake.date_time_this_month()\n",
    "        }\n",
    "        scenarios.append(scenario)\n",
    "    return pd.DataFrame(scenarios)\n",
    "\n",
    "generated_data = generate_scenarios()\n",
    "print(generated_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907eb96-a7ef-4c5d-98a9-8296b51c8c25",
   "metadata": {},
   "source": [
    "FAKER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b0ea8a-809c-42e6-a0e8-cce21a099026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def generate_ride_data(num_samples=10000):\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        data.append({\n",
    "            \"ride_id\": fake.uuid4(),\n",
    "            \"pickup_location\": fake.city(),\n",
    "            \"dropoff_location\": fake.city(),\n",
    "            \"booking_time\": fake.date_time_this_year(),\n",
    "            \"ride_time_minutes\": random.randint(5, 60),\n",
    "            \"driver_rating\": round(random.uniform(3.0, 5.0), 1),\n",
    "            \"passenger_rating\": round(random.uniform(3.0, 5.0), 1),\n",
    "            \"cancellation_reason\": random.choice([\"None\", \"Driver Late\", \"Passenger No-show\", \"Changed Plans\"]),\n",
    "            \"ride_fare\": round(random.uniform(5, 50), 2),\n",
    "            \"cancellation_status\": random.choice([0, 1])  # 0: Completed, 1: Cancelled\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "data = generate_ride_data()\n",
    "data.to_csv(\"ride_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb298636-4c92-4fd4-8347-d044b97109cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['cancellation_status'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c8179-3f7a-4b8e-92f8-7b4047b59c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"ride_data.csv\")\n",
    "\n",
    "# Cancellation Rate\n",
    "cancellation_rate = data['cancellation_status'].mean()\n",
    "print(f\"Cancellation Rate: {cancellation_rate:.2%}\")\n",
    "\n",
    "# Plot cancellation reasons\n",
    "sns.countplot(y=data['cancellation_reason'], order=data['cancellation_reason'].value_counts().index)\n",
    "plt.title(\"Reasons for Cancellation\")\n",
    "plt.show()\n",
    "\n",
    "# # Plot correlation heatmap\n",
    "# sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\")\n",
    "# plt.title(\"Correlation Heatmap\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f7493-5d97-496d-8309-a62307e75d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Encode categorical data\n",
    "le_reason = LabelEncoder()\n",
    "data['cancellation_reason_encoded'] = le_reason.fit_transform(data['cancellation_reason'])\n",
    "\n",
    "# Define features and target\n",
    "features = [\"ride_time_minutes\", \"driver_rating\", \"passenger_rating\", \"ride_fare\", \"cancellation_reason_encoded\"]\n",
    "X = data[features]\n",
    "y = data['cancellation_status']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62f6ae-d626-4134-8624-70135c8c9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee3348-020a-4e50-9d4c-bbc0a18e4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "class RideCancellationModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RideCancellationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = RideCancellationModel(input_size=X_train.shape[1])\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor)\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd412e92-1354-4a1e-8b38-93048777de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Experiment with 0.01 or 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7fdfa4-e32b-4f15-a7ae-e528f5e14448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier  # Example: using Random Forest\n",
    "\n",
    "# Wrap the PyTorch model in a scikit-learn wrapper if necessary\n",
    "rf = RandomForestClassifier()\n",
    "scores = cross_val_score(rf, X, y, cv=5)\n",
    "print(f\"Cross-validation scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea58db-96b7-42ac-aadb-83cfa0e69489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "print(f\"XGBoost Accuracy: {xgb.score(X_test, y_test):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc23b91-d4f6-47dd-a93c-2bb05f8a71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_tensor)\n",
    "    y_test_pred = (y_test_pred > 0.5).float()\n",
    "    accuracy = (y_test_pred.eq(y_test_tensor).sum() / y_test_tensor.shape[0]).item()\n",
    "    print(f\"Test Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57365cbc-47fd-4783-bfe4-f5ddd84fa960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "# def generate_scenario(prompt):\n",
    "#     response = openai.Completion.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         prompt=prompt,\n",
    "#         max_tokens=100\n",
    "#     )\n",
    "#     return response['choices'][0]['text']\n",
    "\n",
    "# scenario_prompt = \"Simulate a ride where the driver cancels due to traffic congestion.\"\n",
    "# print(generate_scenario(scenario_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ba5dd-20c3-4c65-926a-11fa23dc8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "st.title(\"Ride Cancellation Predictor and Optimizer\")\n",
    "\n",
    "# Upload Dataset\n",
    "uploaded_file = st.file_uploader(\"Synthetic_Ride_Data_6000.csv\", type=[\"csv\"])\n",
    "if uploaded_file is not None:\n",
    "    data = pd.read_csv(uploaded_file)\n",
    "    st.write(data.head())\n",
    "#print(st)\n",
    "# Display Metrics\n",
    "st.subheader(\"Model Metrics\")\n",
    "st.write(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "# Plot Data\n",
    "st.subheader(\"Cancellation Analysis\")\n",
    "fig, ax = plt.subplots()\n",
    "data['Cancellation_Status'].value_counts().plot(kind='bar', ax=ax)\n",
    "st.pyplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447eab3-97c2-418a-8558-d58d79e195c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
